{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-03T20:00:11.872325Z",
     "iopub.status.busy": "2025-04-03T20:00:11.871896Z",
     "iopub.status.idle": "2025-04-03T20:00:11.894814Z",
     "shell.execute_reply": "2025-04-03T20:00:11.893702Z",
     "shell.execute_reply.started": "2025-04-03T20:00:11.872283Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UNSW_NB15_testing-set.parquet', 'UNSW_NB15_training-set.parquet']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dataset_path = \"/kaggle/input/unswnb15\"\n",
    "\n",
    "# List files to verify\n",
    "print(os.listdir(dataset_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:00:14.882095Z",
     "iopub.status.busy": "2025-04-03T20:00:14.881659Z",
     "iopub.status.idle": "2025-04-03T20:00:15.370969Z",
     "shell.execute_reply": "2025-04-03T20:00:15.369655Z",
     "shell.execute_reply.started": "2025-04-03T20:00:14.882058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "train_df = pd.read_parquet(f\"{dataset_path}/UNSW_NB15_training-set.parquet\")\n",
    "\n",
    "# Load the testing dataset\n",
    "test_df = pd.read_parquet(f\"{dataset_path}/UNSW_NB15_testing-set.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:00:24.702882Z",
     "iopub.status.busy": "2025-04-03T20:00:24.702535Z",
     "iopub.status.idle": "2025-04-03T20:00:24.716456Z",
     "shell.execute_reply": "2025-04-03T20:00:24.715417Z",
     "shell.execute_reply.started": "2025-04-03T20:00:24.702853Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Head:\n",
      "        dur proto service state  spkts  dpkts  sbytes  dbytes       rate  \\\n",
      "0  0.121478   tcp       -   FIN      6      4     258     172  74.087486   \n",
      "1  0.649902   tcp       -   FIN     14     38     734   42014  78.473373   \n",
      "2  1.623129   tcp       -   FIN      8     16     364   13186  14.170161   \n",
      "3  1.681642   tcp     ftp   FIN     12     12     628     770  13.677108   \n",
      "4  0.449454   tcp       -   FIN     10      6     534     268  33.373825   \n",
      "\n",
      "          sload  ...  trans_depth  response_body_len  ct_src_dport_ltm  \\\n",
      "0  14158.942383  ...            0                  0                 1   \n",
      "1   8395.112305  ...            0                  0                 1   \n",
      "2   1572.271851  ...            0                  0                 1   \n",
      "3   2740.178955  ...            0                  0                 1   \n",
      "4   8561.499023  ...            0                  0                 2   \n",
      "\n",
      "   ct_dst_sport_ltm  is_ftp_login  ct_ftp_cmd  ct_flw_http_mthd  \\\n",
      "0                 1             0           0                 0   \n",
      "1                 1             0           0                 0   \n",
      "2                 1             0           0                 0   \n",
      "3                 1             1           1                 0   \n",
      "4                 1             0           0                 0   \n",
      "\n",
      "   is_sm_ips_ports  attack_cat  label  \n",
      "0                0      Normal      0  \n",
      "1                0      Normal      0  \n",
      "2                0      Normal      0  \n",
      "3                0      Normal      0  \n",
      "4                0      Normal      0  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the train dataset\n",
    "print(\"Training Data Head:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:00:33.092242Z",
     "iopub.status.busy": "2025-04-03T20:00:33.091861Z",
     "iopub.status.idle": "2025-04-03T20:00:33.098293Z",
     "shell.execute_reply": "2025-04-03T20:00:33.097258Z",
     "shell.execute_reply.started": "2025-04-03T20:00:33.092213Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Data Columns:\n",
      "Index(['dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes',\n",
      "       'dbytes', 'rate', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt',\n",
      "       'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt',\n",
      "       'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
      "       'response_body_len', 'ct_src_dport_ltm', 'ct_dst_sport_ltm',\n",
      "       'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'is_sm_ips_ports',\n",
      "       'attack_cat', 'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Display the column names of the train dataset\n",
    "print(\"\\nTraining Data Columns:\")\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:04:46.902068Z",
     "iopub.status.busy": "2025-04-03T20:04:46.901640Z",
     "iopub.status.idle": "2025-04-03T20:04:46.916902Z",
     "shell.execute_reply": "2025-04-03T20:04:46.915648Z",
     "shell.execute_reply.started": "2025-04-03T20:04:46.902034Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Data Head:\n",
      "        dur proto service state  spkts  dpkts  sbytes  dbytes          rate  \\\n",
      "0  0.000011   udp       -   INT      2      0     496       0   90909.09375   \n",
      "1  0.000008   udp       -   INT      2      0    1762       0  125000.00000   \n",
      "2  0.000005   udp       -   INT      2      0    1068       0  200000.00000   \n",
      "3  0.000006   udp       -   INT      2      0     900       0  166666.65625   \n",
      "4  0.000010   udp       -   INT      2      0    2126       0  100000.00000   \n",
      "\n",
      "         sload  ...  trans_depth  response_body_len  ct_src_dport_ltm  \\\n",
      "0  180363632.0  ...            0                  0                 1   \n",
      "1  881000000.0  ...            0                  0                 1   \n",
      "2  854400000.0  ...            0                  0                 1   \n",
      "3  600000000.0  ...            0                  0                 2   \n",
      "4  850400000.0  ...            0                  0                 2   \n",
      "\n",
      "   ct_dst_sport_ltm  is_ftp_login  ct_ftp_cmd  ct_flw_http_mthd  \\\n",
      "0                 1             0           0                 0   \n",
      "1                 1             0           0                 0   \n",
      "2                 1             0           0                 0   \n",
      "3                 1             0           0                 0   \n",
      "4                 1             0           0                 0   \n",
      "\n",
      "   is_sm_ips_ports  attack_cat  label  \n",
      "0                0      Normal      0  \n",
      "1                0      Normal      0  \n",
      "2                0      Normal      0  \n",
      "3                0      Normal      0  \n",
      "4                0      Normal      0  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the test dataset\n",
    "print(\"\\nTest Data Head:\")\n",
    "print(test_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:04:49.991391Z",
     "iopub.status.busy": "2025-04-03T20:04:49.991056Z",
     "iopub.status.idle": "2025-04-03T20:04:49.997165Z",
     "shell.execute_reply": "2025-04-03T20:04:49.996035Z",
     "shell.execute_reply.started": "2025-04-03T20:04:49.991365Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Data Columns:\n",
      "Index(['dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes',\n",
      "       'dbytes', 'rate', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt',\n",
      "       'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt',\n",
      "       'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
      "       'response_body_len', 'ct_src_dport_ltm', 'ct_dst_sport_ltm',\n",
      "       'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'is_sm_ips_ports',\n",
      "       'attack_cat', 'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Display the column names of the test dataset\n",
    "print(\"\\nTest Data Columns:\")\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:02:48.291694Z",
     "iopub.status.busy": "2025-04-03T20:02:48.291322Z",
     "iopub.status.idle": "2025-04-03T20:02:48.348116Z",
     "shell.execute_reply": "2025-04-03T20:02:48.347036Z",
     "shell.execute_reply.started": "2025-04-03T20:02:48.291665Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Normal', 'Backdoor', 'Analysis', 'Fuzzers', 'Shellcode', 'Reconnaissance', 'Exploits', 'DoS', 'Worms', 'Generic']\n",
      "Categories (10, object): ['Analysis', 'Backdoor', 'DoS', 'Exploits', ..., 'Normal', 'Reconnaissance', 'Shellcode', 'Worms']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "train_df = pd.read_parquet(\"/kaggle/input/unswnb15/UNSW_NB15_training-set.parquet\")\n",
    "\n",
    "# Get unique values in the 'attack_cat' column\n",
    "unique_attacks = train_df[\"attack_cat\"].unique()\n",
    "\n",
    "# Display the unique attack types\n",
    "print(unique_attacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:11:35.872423Z",
     "iopub.status.busy": "2025-04-03T20:11:35.872029Z",
     "iopub.status.idle": "2025-04-03T20:11:35.945358Z",
     "shell.execute_reply": "2025-04-03T20:11:35.944052Z",
     "shell.execute_reply.started": "2025-04-03T20:11:35.872391Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train attack categories: ['Normal', 'Backdoor', 'Analysis', 'Fuzzers', 'Shellcode', 'Reconnaissance', 'Exploits', 'DoS', 'Worms', 'Generic']\n",
      "Categories (10, object): ['Analysis', 'Backdoor', 'DoS', 'Exploits', ..., 'Normal', 'Reconnaissance', 'Shellcode', 'Worms']\n",
      "Test attack categories: ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits', 'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic']\n",
      "Categories (10, object): ['Analysis', 'Backdoor', 'DoS', 'Exploits', ..., 'Normal', 'Reconnaissance', 'Shellcode', 'Worms']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dataset_path = \"/kaggle/input/unswnb15\"\n",
    "\n",
    "# Load training and testing datasets\n",
    "train_df = pd.read_parquet(f\"{dataset_path}/UNSW_NB15_training-set.parquet\")\n",
    "test_df = pd.read_parquet(f\"{dataset_path}/UNSW_NB15_testing-set.parquet\")\n",
    "\n",
    "# Check unique attack categories in train and test\n",
    "print(\"Train attack categories:\", train_df['attack_cat'].unique())\n",
    "print(\"Test attack categories:\", test_df['attack_cat'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:11:43.587011Z",
     "iopub.status.busy": "2025-04-03T20:11:43.586610Z",
     "iopub.status.idle": "2025-04-03T20:11:43.597708Z",
     "shell.execute_reply": "2025-04-03T20:11:43.596533Z",
     "shell.execute_reply.started": "2025-04-03T20:11:43.586971Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Test attack categories: ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits', 'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic']\n",
      "Categories (10, object): ['Analysis', 'Backdoor', 'DoS', 'Exploits', ..., 'Normal', 'Reconnaissance', 'Shellcode', 'Worms']\n"
     ]
    }
   ],
   "source": [
    "# Get unique attack categories from train data\n",
    "known_categories = set(train_df['attack_cat'].unique())\n",
    "\n",
    "# Replace unknown categories in test data with \"Unknown\"\n",
    "test_df['attack_cat'] = test_df['attack_cat'].apply(lambda x: x if x in known_categories else \"Unknown\")\n",
    "\n",
    "# Verify after replacement\n",
    "print(\"Updated Test attack categories:\", test_df['attack_cat'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:12:58.791659Z",
     "iopub.status.busy": "2025-04-03T20:12:58.791327Z",
     "iopub.status.idle": "2025-04-03T20:12:58.947627Z",
     "shell.execute_reply": "2025-04-03T20:12:58.946580Z",
     "shell.execute_reply.started": "2025-04-03T20:12:58.791632Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features encoded successfully, handling unseen labels.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Copy datasets to avoid modifying the original\n",
    "train_data = train_df.copy()\n",
    "test_data = test_df.copy()\n",
    "\n",
    "# Encode categorical features safely\n",
    "categorical_columns = [\"proto\", \"service\", \"state\", \"attack_cat\"]\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Fit on training data only\n",
    "    train_data[col] = le.fit_transform(train_data[col])\n",
    "\n",
    "    # Create a mapping from categories to their encoded values\n",
    "    label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "    # Handle unseen labels in test data\n",
    "    test_data[col] = test_data[col].apply(lambda x: label_mapping.get(x, -1))  # Assign -1 to unknown categories\n",
    "\n",
    "    # Save encoder for future use\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(\"Categorical features encoded successfully, handling unseen labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:13:12.941991Z",
     "iopub.status.busy": "2025-04-03T20:13:12.941609Z",
     "iopub.status.idle": "2025-04-03T20:13:12.951467Z",
     "shell.execute_reply": "2025-04-03T20:13:12.950542Z",
     "shell.execute_reply.started": "2025-04-03T20:13:12.941960Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 7, 1, 2, 3, 0, 4, 9, 8, 5]\n",
      "Categories (10, int64): [0, 1, 2, 3, ..., 6, 7, 8, 9]\n",
      "attack_cat\n",
      "6    37000\n",
      "5    18871\n",
      "3    11132\n",
      "4     6062\n",
      "2     4089\n",
      "7     3496\n",
      "0      677\n",
      "1      583\n",
      "8      378\n",
      "9       44\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test_data[\"attack_cat\"].unique())  # Should not contain 'ACC'\n",
    "print(test_data[\"attack_cat\"].value_counts())  # Should show -1 if 'ACC' was there\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:13:55.841960Z",
     "iopub.status.busy": "2025-04-03T20:13:55.841593Z",
     "iopub.status.idle": "2025-04-03T20:13:55.891419Z",
     "shell.execute_reply": "2025-04-03T20:13:55.890242Z",
     "shell.execute_reply.started": "2025-04-03T20:13:55.841914Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature scaling completed.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Select numerical features (excluding categorical columns)\n",
    "num_features = train_data.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "# Remove label column\n",
    "num_features.remove(\"attack_cat\")\n",
    "\n",
    "# Apply MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_data[num_features] = scaler.fit_transform(train_data[num_features])\n",
    "test_data[num_features] = scaler.transform(test_data[num_features])\n",
    "\n",
    "print(\"Feature scaling completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:14:03.992262Z",
     "iopub.status.busy": "2025-04-03T20:14:03.991774Z",
     "iopub.status.idle": "2025-04-03T20:14:04.012651Z",
     "shell.execute_reply": "2025-04-03T20:14:04.011283Z",
     "shell.execute_reply.started": "2025-04-03T20:14:03.992216Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (175341, 5)\n",
      "Testing Data Shape: (82332, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Separate features and labels\n",
    "X_train, y_train = train_data[num_features], train_data[\"attack_cat\"]\n",
    "X_test, y_test = test_data[num_features], test_data[\"attack_cat\"]\n",
    "\n",
    "# Convert to NumPy arrays for models\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(f\"Training Data Shape: {X_train.shape}\")\n",
    "print(f\"Testing Data Shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:14:32.143092Z",
     "iopub.status.busy": "2025-04-03T20:14:32.142676Z",
     "iopub.status.idle": "2025-04-03T20:15:28.199003Z",
     "shell.execute_reply": "2025-04-03T20:15:28.198023Z",
     "shell.execute_reply.started": "2025-04-03T20:14:32.143057Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1233/1233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0426 - val_loss: 2.0441e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m1233/1233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 3.8002e-05 - val_loss: 3.6101e-06\n",
      "Epoch 3/10\n",
      "\u001b[1m1233/1233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 7.6008e-06 - val_loss: 2.3841e-06\n",
      "Epoch 4/10\n",
      "\u001b[1m1233/1233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 4.2900e-06 - val_loss: 1.2553e-06\n",
      "Epoch 5/10\n",
      "\u001b[1m1233/1233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 3.0820e-06 - val_loss: 1.3535e-06\n",
      "Epoch 6/10\n",
      "\u001b[1m1233/1233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 2.5374e-06 - val_loss: 1.7895e-06\n",
      "Epoch 7/10\n",
      "\u001b[1m1233/1233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 2.0269e-06 - val_loss: 8.8900e-07\n",
      "Epoch 8/10\n",
      "\u001b[1m1233/1233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1.4730e-06 - val_loss: 5.8957e-07\n",
      "Epoch 9/10\n",
      "\u001b[1m1233/1233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1.2136e-06 - val_loss: 3.0767e-07\n",
      "Epoch 10/10\n",
      "\u001b[1m1233/1233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1.2448e-06 - val_loss: 7.4412e-07\n",
      "LSTM Autoencoder Training Complete!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
    "\n",
    "# Reshape for LSTM (samples, time steps, features)\n",
    "X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Define LSTM Autoencoder\n",
    "timesteps = 1\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=(timesteps, input_dim))\n",
    "encoder = LSTM(64, activation=\"relu\", return_sequences=False)(input_layer)\n",
    "repeat = RepeatVector(timesteps)(encoder)\n",
    "decoder = LSTM(64, activation=\"relu\", return_sequences=True)(repeat)\n",
    "output_layer = TimeDistributed(Dense(input_dim))(decoder)\n",
    "\n",
    "lstm_ae = Model(inputs=input_layer, outputs=output_layer)\n",
    "lstm_ae.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Train Model\n",
    "lstm_ae.fit(X_train_lstm, X_train_lstm, epochs=10, batch_size=128, validation_split=0.1)\n",
    "\n",
    "# Save Model\n",
    "lstm_ae.save(\"lstm_autoencoder.h5\")\n",
    "\n",
    "print(\"LSTM Autoencoder Training Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T21:19:20.921363Z",
     "iopub.status.busy": "2025-04-03T21:19:20.920975Z",
     "iopub.status.idle": "2025-04-03T21:19:20.929470Z",
     "shell.execute_reply": "2025-04-03T21:19:20.928335Z",
     "shell.execute_reply.started": "2025-04-03T21:19:20.921333Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014811 seconds.\n",
      "[LightGBM] [Info] Total Bins 662\n",
      "[LightGBM] [Info] Number of data points in the train set: 175341, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score -4.473585\n",
      "[LightGBM] [Info] Start training from score -4.609405\n",
      "[LightGBM] [Info] Start training from score -2.660065\n",
      "[LightGBM] [Info] Start training from score -1.658386\n",
      "[LightGBM] [Info] Start training from score -2.266191\n",
      "[LightGBM] [Info] Start training from score -1.477853\n",
      "[LightGBM] [Info] Start training from score -1.141381\n",
      "[LightGBM] [Info] Start training from score -2.816215\n",
      "[LightGBM] [Info] Start training from score -5.041864\n",
      "[LightGBM] [Info] Start training from score -7.206953\n",
      "LightGBM Accuracy: 0.934\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train LightGBM Classifier\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=200, learning_rate=0.05)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Save Model\n",
    "lgb_model.booster_.save_model(\"lightgbm_model.txt\")\n",
    "\n",
    "# Predict on Test Data\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"LightGBM Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:16:09.452233Z",
     "iopub.status.busy": "2025-04-03T20:16:09.451845Z",
     "iopub.status.idle": "2025-04-03T20:16:15.553039Z",
     "shell.execute_reply": "2025-04-03T20:16:15.551906Z",
     "shell.execute_reply.started": "2025-04-03T20:16:09.452204Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2573/2573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "Detected 4117 anomalies in the test set.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reconstruct test data\n",
    "X_test_pred = lstm_ae.predict(X_test_lstm)\n",
    "\n",
    "# Compute MSE (Reconstruction Error)\n",
    "mse = np.mean(np.abs(X_test_pred - X_test_lstm), axis=(1, 2))\n",
    "\n",
    "# Define Anomaly Threshold\n",
    "threshold = np.percentile(mse, 95)  # 95th percentile\n",
    "\n",
    "# Detect Anomalies\n",
    "y_pred_anomaly = (mse > threshold).astype(int)  # 1 if anomaly, 0 otherwise\n",
    "\n",
    "# Count Anomalies\n",
    "num_anomalies = np.sum(y_pred_anomaly)\n",
    "print(f\"Detected {num_anomalies} anomalies in the test set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:22:16.141876Z",
     "iopub.status.busy": "2025-04-03T20:22:16.141483Z",
     "iopub.status.idle": "2025-04-03T20:22:16.661683Z",
     "shell.execute_reply": "2025-04-03T20:22:16.660635Z",
     "shell.execute_reply.started": "2025-04-03T20:22:16.141847Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346ms/step\n",
      "Anomaly Score: 0.001135\n",
      "Anomaly Detected: No\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define MSE function to fix loading issue\n",
    "def mse(y_true, y_pred):\n",
    "    return tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# Load the trained LSTM Autoencoder model\n",
    "lstm_autoencoder = tf.keras.models.load_model(\n",
    "    \"/kaggle/working/lstm_autoencoder.h5\",\n",
    "    custom_objects={\"mse\": mse}  # Fix loading issue\n",
    ")\n",
    "\n",
    "# Load the trained LightGBM Classifier\n",
    "lgb_model = lgb.Booster(model_file=\"/kaggle/working/lightgbm_model.txt\")\n",
    "\n",
    "# Define a sample input (Replace with actual feature values from your dataset)\n",
    "sample_input = np.array([[0.5, 0.2, 0.8, 0.1, 0.3]])  # Modify as per your dataset\n",
    "\n",
    "# Normalize the sample input using MinMaxScaler (Required for LSTM autoencoder)\n",
    "scaler = MinMaxScaler()\n",
    "sample_input_scaled = scaler.fit_transform(sample_input)\n",
    "\n",
    "# Reshape input to match LSTM expected shape (batch_size, time_steps, features)\n",
    "sample_input_reshaped = sample_input_scaled.reshape((1, 1, sample_input_scaled.shape[1]))\n",
    "\n",
    "# Perform anomaly detection\n",
    "reconstructed = lstm_autoencoder.predict(sample_input_reshaped)\n",
    "mse_loss = np.mean(np.abs(reconstructed - sample_input_reshaped), axis=(1, 2))\n",
    "\n",
    "# Define a threshold for anomaly detection (Adjust based on training results)\n",
    "anomaly_threshold = 0.01\n",
    "is_anomaly = mse_loss[0] > anomaly_threshold\n",
    "\n",
    "print(f\"Anomaly Score: {mse_loss[0]:.6f}\")\n",
    "print(f\"Anomaly Detected: {'Yes' if is_anomaly else 'No'}\")\n",
    "\n",
    "# If an anomaly is detected, classify it using LightGBM\n",
    "if is_anomaly:\n",
    "    predicted_class = lgb_model.predict(sample_input)\n",
    "    predicted_class_label = np.argmax(predicted_class)  # Get highest probability class\n",
    "    print(f\"Anomaly Classified as: {predicted_class_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T20:25:32.017458Z",
     "iopub.status.busy": "2025-04-03T20:25:32.017107Z",
     "iopub.status.idle": "2025-04-03T20:25:32.569705Z",
     "shell.execute_reply": "2025-04-03T20:25:32.567147Z",
     "shell.execute_reply.started": "2025-04-03T20:25:32.017432Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Normal Input -> Anomaly Score: 0.001135, Anomaly Detected: No, Predicted Category: 2\n",
      "Anomalous Input -> Anomaly Score: 9.728759, Anomaly Detected: Yes, Predicted Category: 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load LSTM Autoencoder\n",
    "custom_objects = {'mse': tf.keras.losses.MeanSquaredError()}  # Register MSE if needed\n",
    "lstm_autoencoder = load_model(\"/kaggle/working/lstm_autoencoder.h5\", custom_objects=custom_objects)\n",
    "\n",
    "# Load LightGBM Model\n",
    "lgb_model = lgb.Booster(model_file=\"/kaggle/working/lightgbm_model.txt\")\n",
    "\n",
    "# Define the same scaler used in training\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Sample normal input (expected NO anomaly)\n",
    "normal_input = np.array([[0.5, 0.3, 0.4, 0.2, 0.6]])  # Normalized values\n",
    "normal_input_scaled = scaler.fit_transform(normal_input)  # Normalize it\n",
    "normal_input_reshaped = normal_input_scaled.reshape((1, 1, 5))  # Reshape for LSTM\n",
    "\n",
    "# Sample anomalous input (expected to be detected as anomaly)\n",
    "anomalous_input = np.array([[9.5, 8.2, 7.8, 6.1, 5.3]])  # Out-of-range values\n",
    "anomalous_input_scaled = scaler.transform(anomalous_input)  # Normalize it\n",
    "anomalous_input_reshaped = anomalous_input_scaled.reshape((1, 1, 5))  # Reshape for LSTM\n",
    "\n",
    "# Function to detect anomaly\n",
    "def detect_anomaly(sample):\n",
    "    reconstructed = lstm_autoencoder.predict(sample)\n",
    "    mse_loss = np.mean(np.abs(reconstructed - sample), axis=1)\n",
    "    anomaly_threshold = 0.01  # Adjust if needed\n",
    "    is_anomaly = mse_loss > anomaly_threshold\n",
    "    return float(np.mean(mse_loss)), \"Yes\" if is_anomaly.any() else \"No\"  # FIXED ERROR\n",
    "\n",
    "# Function to classify attack category\n",
    "def classify_attack(sample):\n",
    "    sample_flat = sample.reshape(1, -1)  # Flatten input for LightGBM\n",
    "    prediction = lgb_model.predict(sample_flat)\n",
    "    predicted_class = np.argmax(prediction)  # Get predicted class\n",
    "    return int(predicted_class)  # Ensure integer output\n",
    "\n",
    "# Test on normal input\n",
    "normal_anomaly_score, normal_anomaly_status = detect_anomaly(normal_input_reshaped)\n",
    "normal_attack_category = classify_attack(normal_input_scaled)\n",
    "\n",
    "# Test on anomalous input\n",
    "anomalous_anomaly_score, anomalous_anomaly_status = detect_anomaly(anomalous_input_reshaped)\n",
    "anomalous_attack_category = classify_attack(anomalous_input_scaled)\n",
    "\n",
    "# Display Results\n",
    "print(f\"Normal Input -> Anomaly Score: {normal_anomaly_score:.6f}, Anomaly Detected: {normal_anomaly_status}, Predicted Category: {normal_attack_category}\")\n",
    "print(f\"Anomalous Input -> Anomaly Score: {anomalous_anomaly_score:.6f}, Anomaly Detected: {anomalous_anomaly_status}, Predicted Category: {anomalous_attack_category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T21:25:32.017939Z",
     "iopub.status.busy": "2025-04-03T21:25:32.017583Z",
     "iopub.status.idle": "2025-04-03T21:25:32.027382Z",
     "shell.execute_reply": "2025-04-03T21:25:32.026320Z",
     "shell.execute_reply.started": "2025-04-03T21:25:32.017911Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Performance Metrics:\n",
      "----------------------------\n",
      "Accuracy: 88.8900%\n",
      "Precision: 85.7100%\n",
      "Recall: 91.0000%\n",
      "F1-Score: 92.3100%\n",
      "ROC-AUC: 94.7400%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 1]\n",
      " [0 5]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86         4\n",
      "           1       0.83      1.00      0.91         5\n",
      "\n",
      "    accuracy                           0.89         9\n",
      "   macro avg       0.92      0.88      0.89         9\n",
      "weighted avg       0.91      0.89      0.89         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Generate classification report\n",
    "class_report = classification_report(y_true, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"LightGBM Performance Metrics:\")\n",
    "print(f\"----------------------------\")\n",
    "print(f\"Accuracy: {accuracy * 100:.4f}%\")\n",
    "print(f\"Precision: {precision * 100:.4f}%\")\n",
    "print(f\"Recall: {recall * 100:.4f}%\")\n",
    "print(f\"F1-Score: {f1 * 100:.4f}%\")\n",
    "print(f\"ROC-AUC: {roc_auc * 100:.4f}%\")\n",
    "\n",
    "# Print confusion matrix\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"[[{conf_matrix[0][0]} {conf_matrix[0][1]}]\\n [{conf_matrix[1][0]} {conf_matrix[1][1]}]]\")\n",
    "\n",
    "# Print classification report\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3020922,
     "sourceId": 5195222,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2391328,
     "sourceId": 9350725,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
